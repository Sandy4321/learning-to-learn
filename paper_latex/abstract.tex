People use rich prior knowledge about the world in order 
to efficiently learn new concepts. These priors--commonly referred to as
"inductive biases"--pertain to the space of internal models considered by a
learner, and they help maximize the amount of information that is extracted
from limited data. Recently, it was shown that performance-optimized 
deep neural networks (DNNs) develop inductive biases similar to those 
possessed by human children. However, these models use unrealistic training
data, and it remains unclear whether they develop their biases in the same way
as humans. We investigate the development of inductive biases in DNNs and
perform novel regional parametric analyses of these biases. Our findings
suggest...

\textbf{Keywords:}
learning-to-learn; neural networks; inductive biases