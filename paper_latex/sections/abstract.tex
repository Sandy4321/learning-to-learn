People use rich prior knowledge about the world in order
to efficiently learn new concepts. These priors--commonly referred to as
``inductive biases"--pertain to the space of internal models considered by a
learner, and they help maximize the amount of information that is extracted
from limited data. Recently, it was shown that performance-optimized
deep neural networks (DNNs) develop inductive biases similar to those
possessed by human children. However, these models use unrealistic training
data, and it remains unclear whether they develop their biases in the same way
as humans. We investigate the development and influence of inductive biases
in DNNs using an experimental paradigm borrowed from develpmental psychology.
We find that simple neural network models can learn simplifying inductive
biases from as few as 2 examples of each concept, and that these biases tend
to grow with depth in the network. The development of these biases predicts
the onset of vocabulary acceleration in our networks, similar to what
has been found in children.

\textbf{Keywords:}
learning-to-learn; neural networks; inductive biases